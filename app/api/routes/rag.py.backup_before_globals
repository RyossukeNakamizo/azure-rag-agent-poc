"""
RAG API Routes

Phase 2-3: RAG エンドポイント実装
"""
import logging
import json
from typing import AsyncGenerator

from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse

from app.core.config import get_settings, Settings
from app.services.search_service import SearchService
from app.services.foundry_agent import FoundryAgentService
from app.models.rag import (
    RAGSearchRequest,
    RAGSearchResponse,
    SearchResult,
    RAGChatRequest,
    RAGChatResponse,
    SourceReference,
    RAGHealthResponse,
)

logger = logging.getLogger(__name__)

router = APIRouter()

# デフォルトシステムプロンプト
DEFAULT_RAG_SYSTEM_PROMPT = """あなたはAzure技術の専門家です。
以下のルールに従って回答してください：
1. 提供されたコンテキストのみを使用して回答する
2. コンテキストに情報がない場合は、その旨を明示する
3. 回答には必ずソースを引用する
4. 技術的に正確かつ実用的な回答を心がける
5. 日本語で回答する"""


def get_agent_service():
    """FoundryAgentService依存性注入"""
    if not _agent_service:
        raise HTTPException(
            status_code=500, detail="FoundryAgentService not initialized"
        )
    return _agent_service




@router.get("/health", response_model=RAGHealthResponse)
async def health_check(
    search_service: SearchService = Depends(get_search_service),
    agent_service: FoundryAgentService = Depends(get_agent_service),
    settings: Settings = Depends(get_settings),
):
    """RAG システム Health Check"""
    search_status = "unknown"
    openai_status = "unknown"
    
    # Search サービス確認
    try:
        results = search_service.keyword_search("test", top_k=1)
        search_status = "healthy"
    except Exception as e:
        search_status = f"unhealthy: {str(e)[:50]}"
        logger.error(f"Search health check failed: {e}")
    
    # OpenAI サービス確認
    try:
        embedding = agent_service.get_embedding("test")
        if embedding and len(embedding) == 1536:
            openai_status = "healthy"
        else:
            openai_status = "unhealthy: invalid embedding"
    except Exception as e:
        openai_status = f"unhealthy: {str(e)[:50]}"
        logger.error(f"OpenAI health check failed: {e}")
    
    overall = "healthy" if search_status == "healthy" and openai_status == "healthy" else "degraded"
    
    return RAGHealthResponse(
        status=overall,
        search_service=search_status,
        index_name=settings.AZURE_SEARCH_INDEX,
        openai_service=openai_status,
    )


@router.post("/search", response_model=RAGSearchResponse)
async def hybrid_search(
    request: RAGSearchRequest,
    search_service: SearchService = Depends(get_search_service),
    agent_service: FoundryAgentService = Depends(get_agent_service),
):
    """ハイブリッド検索（Vector + Keyword）"""
    try:
        logger.info(f"Search request: query='{request.query}', top_k={request.top_k}")
        
        # Embedding 生成
        query_embedding = agent_service.get_embedding(request.query)
        if not query_embedding:
            raise HTTPException(status_code=500, detail="Failed to generate embedding")
        
        # 検索実行
        raw_results = search_service.hybrid_search(
            query=request.query,
            embedding=query_embedding,
            top_k=request.top_k,
            filter_expression=request.filter,
        )
        
        results = [
            SearchResult(
                id=r.get("id", ""),
                title=r.get("title", ""),
                content=r.get("content", ""),
                chunk_id=r.get("chunk_id"),
                score=r.get("score", 0.0),
            )
            for r in raw_results
        ]
        
        return RAGSearchResponse(
            query=request.query,
            results=results,
            total_count=len(results),
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Search error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat", response_model=RAGChatResponse)
async def rag_chat(
    request: RAGChatRequest,
    search_service: SearchService = Depends(get_search_service),
    agent_service: FoundryAgentService = Depends(get_agent_service),
    settings: Settings = Depends(get_settings),
):
    """RAG Chat（検索 + 回答生成）"""
    try:
        logger.info(f"RAG chat: message='{request.message[:50]}...'")
        
        # Step 1: Embedding
        query_embedding = agent_service.get_embedding(request.message)
        if not query_embedding:
            raise HTTPException(status_code=500, detail="Failed to generate embedding")
        
        # Step 2: 検索
        search_results = search_service.hybrid_search(
            query=request.message,
            embedding=query_embedding,
            top_k=request.top_k,
            filter_expression=request.filter,
        )
        
        # Step 3: コンテキスト構築
        if search_results:
            context_parts = []
            for i, doc in enumerate(search_results, 1):
                title = doc.get("title", "Untitled")
                content = doc.get("content", "")
                context_parts.append(f"【ソース{i}: {title}】\n{content}")
            context_text = "\n\n---\n\n".join(context_parts)
        else:
            context_text = "（関連するコンテキストが見つかりませんでした）"
        
        # Step 4: プロンプト
        system_prompt = request.system_prompt or DEFAULT_RAG_SYSTEM_PROMPT
        user_message = f"""以下のコンテキストを参照して、質問に回答してください。

【コンテキスト】
{context_text}

【質問】
{request.message}"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ]
        
        # Step 5: LLM呼び出し
        response = agent_service.chat(
            messages=messages,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            stream=False,
        )
        
        answer = response.choices[0].message.content if response.choices else "回答を生成できませんでした"
        
        sources = [
            SourceReference(
                id=doc.get("id", ""),
                title=doc.get("title", ""),
                score=doc.get("score", 0.0),
            )
            for doc in search_results
        ]
        
        return RAGChatResponse(
            answer=answer,
            sources=sources,
            context_used=len(search_results),
            model=settings.AZURE_OPENAI_DEPLOYMENT_CHAT,
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"RAG chat error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/stream")
async def rag_chat_stream(
    request: RAGChatRequest,
    search_service: SearchService = Depends(get_search_service),
    agent_service: FoundryAgentService = Depends(get_agent_service),
):
    """RAG Chat ストリーミング版"""
    try:
        # Embedding
        query_embedding = agent_service.get_embedding(request.message)
        if not query_embedding:
            raise HTTPException(status_code=500, detail="Failed to generate embedding")
        
        # 検索
        search_results = search_service.hybrid_search(
            query=request.message,
            embedding=query_embedding,
            top_k=request.top_k,
            filter_expression=request.filter,
        )
        
        # コンテキスト
        if search_results:
            context_parts = []
            for i, doc in enumerate(search_results, 1):
                title = doc.get("title", "Untitled")
                content = doc.get("content", "")
                context_parts.append(f"【ソース{i}: {title}】\n{content}")
            context_text = "\n\n---\n\n".join(context_parts)
        else:
            context_text = "（関連するコンテキストが見つかりませんでした）"
        
        system_prompt = request.system_prompt or DEFAULT_RAG_SYSTEM_PROMPT
        user_message = f"""以下のコンテキストを参照して、質問に回答してください。

【コンテキスト】
{context_text}

【質問】
{request.message}"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ]
        
        async def generate() -> AsyncGenerator[str, None]:
            # ソース情報送信
            sources_data = {
                "type": "sources",
                "sources": [
                    {"id": doc.get("id", ""), "title": doc.get("title", ""), "score": doc.get("score", 0.0)}
                    for doc in search_results
                ],
                "context_used": len(search_results),
            }
            yield f"data: {json.dumps(sources_data, ensure_ascii=False)}\n\n"
            
            # ストリーミング
            stream = agent_service.chat(
                messages=messages,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=True,
            )
            
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    yield f"data: {json.dumps({'type': 'content', 'content': content}, ensure_ascii=False)}\n\n"
            
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"RAG stream error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
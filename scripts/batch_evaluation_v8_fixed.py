#!/usr/bin/env python3
"""
Batch Evaluation v8: Live Azure AI Search Integration (ä¿®æ­£ç‰ˆ)
"""

import os
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

from azure.identity import DefaultAzureCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizedQuery
from openai import AzureOpenAI


class RAGEvaluator:
    """RAGè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.search_endpoint = os.getenv("AZURE_SEARCH_ENDPOINT")
        self.search_index = os.getenv("AZURE_SEARCH_INDEX", "rag-index")
        self.openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        self.deployment_chat = os.getenv("AZURE_OPENAI_DEPLOYMENT_CHAT", "gpt-4o")
        self.deployment_embedding = os.getenv("AZURE_OPENAI_DEPLOYMENT_EMBEDDING", "text-embedding-ada-002")
        
        self.credential = DefaultAzureCredential()
        
        self.search_client = SearchClient(
            endpoint=self.search_endpoint,
            index_name=self.search_index,
            credential=self.credential
        )
        
        self.openai_client = AzureOpenAI(
            azure_endpoint=self.openai_endpoint,
            azure_ad_token_provider=self._get_token,
            api_version="2024-10-01-preview"
        )
    
    def _get_token(self) -> str:
        return self.credential.get_token(
            "https://cognitiveservices.azure.com/.default"
        ).token
    
    def get_embedding(self, text: str) -> List[float]:
        response = self.openai_client.embeddings.create(
            model=self.deployment_embedding,
            input=text
        )
        return response.data[0].embedding
    
    def hybrid_search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        embedding = self.get_embedding(query)
        
        vector_query = VectorizedQuery(
            vector=embedding,
            k_nearest_neighbors=top_k,
            fields="content_vector"  # ä¿®æ­£: contentVector â†’ content_vector
        )
        
        results = self.search_client.search(
            search_text=query,
            vector_queries=[vector_query],
            select=["id", "title", "content", "category"],
            top=top_k
        )
        
        return [
            {
                "id": r.get("id", ""),
                "title": r.get("title", ""),
                "content": r.get("content", ""),
                "category": r.get("category", ""),
                "score": r.get("@search.score", 0.0)
            }
            for r in results
        ]
    
    def generate_answer(self, query: str, context: List[Dict[str, Any]]) -> str:
        # app/api/routes/rag.pyã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        import sys
        sys.path.insert(0, str(Path(__file__).parent.parent))
        from app.api.routes.rag import DEFAULT_RAG_SYSTEM_PROMPT
        
        context_text = "\n\n".join([
            f"ã€{c['title']}ã€‘\n{c['content']}"
            for c in context
        ])
        
        user_message = f"""ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
{context_text}

ã€è³ªå•ã€‘
{query}"""
        
        messages = [
            {"role": "system", "content": DEFAULT_RAG_SYSTEM_PROMPT},
            {"role": "user", "content": user_message},
        ]
        
        response = self.openai_client.chat.completions.create(
            model=self.deployment_chat,
            messages=messages,
            temperature=0.3,
            max_tokens=1000,
        )
        
        return response.choices[0].message.content
    
    def evaluate_metric(self, query: str, answer: str, context: List[Dict[str, Any]], metric: str) -> float:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡ï¼ˆLLM-as-Judgeï¼‰"""
        
        metric_prompts = {
            "coherence": """ä»¥ä¸‹ã®å›ç­”ã®è«–ç†çš„ä¸€è²«æ€§ã‚’0-1ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

è³ªå•: {query}
å›ç­”: {answer}

è©•ä¾¡åŸºæº–:
- 1.0: å®Œå…¨ã«è«–ç†çš„ã§çŸ›ç›¾ãŒãªã„
- 0.5: éƒ¨åˆ†çš„ã«è«–ç†çš„ã ãŒæ”¹å–„ã®ä½™åœ°ã‚ã‚Š
- 0.0: è«–ç†çš„ã§ãªã„ã€çŸ›ç›¾ãŒå¤šã„

ã‚¹ã‚³ã‚¢ã®ã¿ã‚’æ•°å€¤ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 0.85ï¼‰""",
            
            "relevance": """ä»¥ä¸‹ã®å›ç­”ãŒè³ªå•ã«å¯¾ã—ã¦ã©ã®ç¨‹åº¦é–¢é€£æ€§ãŒã‚ã‚‹ã‹ã‚’0-1ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

è³ªå•: {query}
å›ç­”: {answer}

è©•ä¾¡åŸºæº–:
- 1.0: å®Œå…¨ã«è³ªå•ã«ç­”ãˆã¦ã„ã‚‹
- 0.5: éƒ¨åˆ†çš„ã«é–¢é€£ã—ã¦ã„ã‚‹
- 0.0: è³ªå•ã¨ç„¡é–¢ä¿‚

ã‚¹ã‚³ã‚¢ã®ã¿ã‚’æ•°å€¤ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 0.92ï¼‰""",
            
            "groundedness": """ä»¥ä¸‹ã®å›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã‚’0-1ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {context}
å›ç­”: {answer}

è©•ä¾¡åŸºæº–:
- 1.0: å›ç­”ã®å…¨ã¦ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹
- 0.5: ä¸€éƒ¨ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤–ã®æƒ…å ±ã‚’å«ã‚€
- 0.0: ã»ã¨ã‚“ã©ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤–ã®æƒ…å ±

ã‚¹ã‚³ã‚¢ã®ã¿ã‚’æ•°å€¤ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 0.78ï¼‰"""
        }
        
        context_text = "\n".join([c['content'][:200] for c in context])  # é•·ã•åˆ¶é™
        prompt = metric_prompts[metric].format(
            query=query,
            answer=answer,
            context=context_text
        )
        
        response = self.openai_client.chat.completions.create(
            model=self.deployment_chat,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=10,
        )
        
        try:
            score = float(response.choices[0].message.content.strip())
            return max(0.0, min(1.0, score))
        except:
            return 0.0


def main():
    parser = argparse.ArgumentParser(description="RAGè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  v8")
    parser.add_argument("--qa-data", required=True, help="è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆJSONLå½¢å¼ï¼‰")
    parser.add_argument("--output-dir", default="evaluation_results/D21-4", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°")
    
    args = parser.parse_args()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆJSONLå¯¾å¿œï¼‰
    qa_pairs = []
    with open(args.qa_data, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                qa_pairs.append(json.loads(line))
    
    print(f"ğŸ“Š è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(qa_pairs)}")
    
    # è©•ä¾¡å®Ÿè¡Œ
    evaluator = RAGEvaluator()
    results = []
    
    for i, qa in enumerate(qa_pairs, 1):
        query = qa.get("question", qa.get("query", ""))
        
        if args.verbose:
            print(f"\n[{i}/{len(qa_pairs)}] è³ªå•: {query[:50]}...")
        
        try:
            # RAGå®Ÿè¡Œ
            context = evaluator.hybrid_search(query, top_k=3)
            answer = evaluator.generate_answer(query, context)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡
            coherence = evaluator.evaluate_metric(query, answer, context, "coherence")
            relevance = evaluator.evaluate_metric(query, answer, context, "relevance")
            groundedness = evaluator.evaluate_metric(query, answer, context, "groundedness")
            
            results.append({
                "question": query,
                "answer": answer,
                "context_count": len(context),
                "coherence": coherence,
                "relevance": relevance,
                "groundedness": groundedness,
            })
            
            if args.verbose:
                print(f"  Coherence: {coherence:.3f}, Relevance: {relevance:.3f}, Groundedness: {groundedness:.3f}")
        
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    # é›†è¨ˆ
    avg_coherence = sum(r["coherence"] for r in results) / len(results) if results else 0
    avg_relevance = sum(r["relevance"] for r in results) / len(results) if results else 0
    avg_groundedness = sum(r["groundedness"] for r in results) / len(results) if results else 0
    
    # çµæœä¿å­˜
    output_file = output_dir / "evaluation_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "total_questions": len(qa_pairs),
            "successful_evaluations": len(results),
            "aggregate_metrics": {
                "coherence": avg_coherence,
                "relevance": avg_relevance,
                "groundedness": avg_groundedness,
            },
            "results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… è©•ä¾¡å®Œäº†")
    print(f"æˆåŠŸ: {len(results)}/{len(qa_pairs)}")
    print(f"å¹³å‡ Coherence: {avg_coherence:.3f}")
    print(f"å¹³å‡ Relevance: {avg_relevance:.3f}")
    print(f"å¹³å‡ Groundedness: {avg_groundedness:.3f}")
    print(f"\nçµæœ: {output_file}")


if __name__ == "__main__":
    main()

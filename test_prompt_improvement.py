"""D21-4 Prompt Engineeringæ”¹å–„åŠ¹æœã®ç°¡æ˜“ãƒ†ã‚¹ãƒˆ"""
import os
import sys
from dotenv import load_dotenv
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.services.search_service import SearchService

load_dotenv()

# Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
credential = DefaultAzureCredential()
openai_client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    azure_ad_token_provider=lambda: credential.get_token("https://cognitiveservices.azure.com/.default").token,
    api_version="2024-10-01-preview"
)

# ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
search_service = SearchService(
    endpoint=os.getenv("AZURE_SEARCH_ENDPOINT"),
    index_name=os.getenv("AZURE_SEARCH_INDEX"),
)

def get_embedding(text: str) -> list[float]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
    response = openai_client.embeddings.create(
        model=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-ada-002"),
        input=text
    )
    return response.data[0].embedding

# ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
test_queries = [
    "Azure AI Searchã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
    "Bicepã§ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ•™ãˆã¦",
]

def test_rag(query: str):
    """RAGãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print(f"\n{'='*80}")
    print(f"è³ªå•: {query}")
    print(f"{'='*80}\n")
    
    # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
    embedding = get_embedding(query)
    
    # æ¤œç´¢å®Ÿè¡Œ
    results = search_service.hybrid_search(
        query=query,
        embedding=embedding,
        top_k=3
    )
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
    context_text = "\n\n".join([
        f"ã€{doc.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')}ã€‘\n{doc.get('content', '')}"
        for doc in results
    ])
    
    # app/api/routes/rag.pyã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from app.api.routes.rag import DEFAULT_RAG_SYSTEM_PROMPT
    
    user_message = f"""ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
{context_text}

ã€è³ªå•ã€‘
{query}"""
    
    messages = [
        {"role": "system", "content": DEFAULT_RAG_SYSTEM_PROMPT},
        {"role": "user", "content": user_message},
    ]
    
    # LLMå‘¼ã³å‡ºã—ï¼ˆOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç›´æ¥ä½¿ç”¨ï¼‰
    response = openai_client.chat.completions.create(
        model=os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o"),
        messages=messages,
        temperature=0.3,
        max_tokens=1000,
    )
    
    answer = response.choices[0].message.content
    
    print(f"å›ç­”:\n{answer}\n")
    print(f"æ¤œç´¢çµæœæ•°: {len(results)}")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    print("ğŸš€ D21-4 Prompt Engineeringæ”¹å–„ãƒ†ã‚¹ãƒˆé–‹å§‹\n")
    for query in test_queries:
        try:
            test_rag(query)
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}\n")
            import traceback
            traceback.print_exc()
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
